"""
–ü–∞—Ä—Å–µ—Ä –¥–∞–Ω–Ω—ã—Ö –æ —Ä–µ–¥–∫–æ—Å—Ç–∏ –ø—Ä–µ–¥–º–µ—Ç–æ–≤ —Å —Å–∞–π—Ç–∞ growagardenpro.com
"""

import asyncio
import re
from typing import Dict, Optional, Set
from dataclasses import dataclass

try:
    import aiohttp
    from bs4 import BeautifulSoup
    from loguru import logger
except ImportError:
    import logging
    aiohttp = None
    BeautifulSoup = None
    logger = logging.getLogger(__name__)

from roblox_garden.models.shop import Rarity, ItemType


@dataclass
class CropInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—É–ª—å—Ç—É—Ä–µ —Å —Å–∞–π—Ç–∞."""
    name: str
    rarity: Rarity
    item_type: ItemType
    base_value: int
    available: bool = True


class RarityParser:
    """–ü–∞—Ä—Å–µ—Ä –¥–∞–Ω–Ω—ã—Ö –æ —Ä–µ–¥–∫–æ—Å—Ç–∏ —Å growagardenpro.com"""
    
    BASE_URL = "https://growagardenpro.com"
    ENDPOINTS = {
        "crops": "/crops/",
        "gear": "/gear/", 
        "eggs": "/eggs/",
        "cosmetics": "/cosmetics/"
    }
    
    # –ú–∞–ø–ø–∏–Ω–≥ —Å—Ç—Ä–æ–∫ —Ä–µ–¥–∫–æ—Å—Ç–∏ –≤ enum
    RARITY_MAPPING = {
        "Common": Rarity.COMMON,
        "Uncommon": Rarity.UNCOMMON, 
        "Rare": Rarity.RARE,
        "Legendary": Rarity.LEGENDARY,
        "Mythical": Rarity.MYTHICAL,
        "Divine": Rarity.DIVINE,
        "Prismatic": Rarity.PRISMATIC,
        "Unknown": Rarity.COMMON  # Fallback –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö
    }
    
    # –ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ —Ç–∏–ø—ã –ø—Ä–µ–¥–º–µ—Ç–æ–≤
    TYPE_MAPPING = {
        "crops": ItemType.SEED,
        "gear": ItemType.GEAR,
        "eggs": ItemType.EGG,
        "cosmetics": ItemType.COSMETIC
    }
    
    def __init__(self):
        self.session = None
        self._cached_data: Dict[str, CropInfo] = {}
        self._last_update = None
        self.cache_ttl = 3600  # 1 —á–∞—Å
    
    async def __aenter__(self):
        """Async context manager entry."""
        if aiohttp is None:
            raise ImportError("aiohttp is required for RarityParser")
        
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'Mozilla/5.0 (Roblox Garden Parser)'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self.session:
            await self.session.close()
    
    async def fetch_all_rarities(self) -> Dict[str, CropInfo]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ —Ä–µ–¥–∫–æ—Å—Ç–∏ –≤—Å–µ—Ö –ø—Ä–µ–¥–º–µ—Ç–æ–≤."""
        if not self._should_refresh_cache():
            return self._cached_data
            
        logger.info("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ —Ä–µ–¥–∫–æ—Å—Ç–∏ —Å growagardenpro.com...")
        
        all_items = {}
        
        for category, endpoint in self.ENDPOINTS.items():
            try:
                items = await self._fetch_category_data(category, endpoint)
                all_items.update(items)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(items)} –ø—Ä–µ–¥–º–µ—Ç–æ–≤ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category}")
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {category}: {e}")
                continue
        
        self._cached_data = all_items
        self._last_update = asyncio.get_event_loop().time()
        
        logger.info(f"üéØ –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_items)} –ø—Ä–µ–¥–º–µ—Ç–æ–≤ —Å –¥–∞–Ω–Ω—ã–º–∏ –æ —Ä–µ–¥–∫–æ—Å—Ç–∏")
        return all_items
    
    async def _fetch_category_data(self, category: str, endpoint: str) -> Dict[str, CropInfo]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
        if not self.session:
            raise RuntimeError("Session not initialized")
            
        url = f"{self.BASE_URL}{endpoint}"
        
        async with self.session.get(url) as response:
            if response.status != 200:
                raise Exception(f"HTTP {response.status} for {url}")
            
            html = await response.text()
            
        return self._parse_category_html(html, category)
    
    def _parse_category_html(self, html: str, category: str) -> Dict[str, CropInfo]:
        """–ü–∞—Ä—Å–∏–Ω–≥ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
        if BeautifulSoup is None:
            return self._parse_with_regex(html, category)
        
        return self._parse_with_bs4(html, category)
    
    def _parse_with_regex(self, html: str, category: str) -> Dict[str, CropInfo]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π (fallback)."""
        items = {}
        item_type = self.TYPE_MAPPING.get(category, ItemType.SEED)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–µ–¥–º–µ—Ç–æ–≤: [Rarity] [Type] Name Name ...
        pattern = r'\[(\w+)\]\s*(?:\w+\s+)?([^[]+?)(?:Base Value:|Seed Price:|Available:)'
        
        matches = re.findall(pattern, html, re.IGNORECASE)
        
        for rarity_str, name_part in matches:
            # –û—á–∏—Å—Ç–∫–∞ –∏–º–µ–Ω–∏ –ø—Ä–µ–¥–º–µ—Ç–∞
            name = re.sub(r'\s+', ' ', name_part).strip()
            name = re.sub(r'(Multi|Limited|Single)\s*', '', name).strip()
            
            if not name or len(name) < 2:
                continue
                
            rarity = self.RARITY_MAPPING.get(rarity_str, Rarity.COMMON)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ base value
            value_match = re.search(rf'{re.escape(name)}.*?Base Value:\s*Sheckle\s*([\d,]+)', html)
            base_value = 0
            if value_match:
                base_value = int(value_match.group(1).replace(',', ''))
            
            items[name] = CropInfo(
                name=name,
                rarity=rarity,
                item_type=item_type,
                base_value=base_value,
                available=True
            )
        
        return items
    
    def _parse_with_bs4(self, html: str, category: str) -> Dict[str, CropInfo]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å –ø–æ–º–æ—â—å—é BeautifulSoup."""
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º regex fallback, —Ç–∞–∫ –∫–∞–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–π—Ç–∞ —Å–ª–æ–∂–Ω–∞—è
        return self._parse_with_regex(html, category)
    
    def _should_refresh_cache(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å –∫–µ—à."""
        if not self._cached_data or self._last_update is None:
            return True
            
        current_time = asyncio.get_event_loop().time()
        return (current_time - self._last_update) > self.cache_ttl
    
    def get_item_rarity(self, item_name: str) -> Optional[Rarity]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–¥–∫–æ—Å—Ç—å –ø—Ä–µ–¥–º–µ—Ç–∞ –ø–æ –∏–º–µ–Ω–∏."""
        crop_info = self._cached_data.get(item_name)
        return crop_info.rarity if crop_info else None
    
    def get_item_info(self, item_name: str) -> Optional[CropInfo]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–µ–¥–º–µ—Ç–µ."""
        return self._cached_data.get(item_name)
    
    async def get_divine_plus_items(self) -> Set[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–º–µ—Ç–æ–≤ Divine —Ä–µ–¥–∫–æ—Å—Ç–∏ –∏ –≤—ã—à–µ."""
        await self.fetch_all_rarities()
        
        divine_plus = {Rarity.DIVINE, Rarity.PRISMATIC}
        
        return {
            name for name, info in self._cached_data.items()
            if info.rarity in divine_plus
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏
_global_parser: Optional[RarityParser] = None


async def get_rarity_parser() -> RarityParser:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞."""
    global _global_parser
    
    if _global_parser is None:
        _global_parser = RarityParser()
        async with _global_parser:
            await _global_parser.fetch_all_rarities()
    
    return _global_parser


async def get_item_rarity(item_name: str) -> Optional[Rarity]:
    """–ë—ã—Å—Ç—Ä—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–¥–∫–æ—Å—Ç—å –ø—Ä–µ–¥–º–µ—Ç–∞."""
    parser = await get_rarity_parser()
    return parser.get_item_rarity(item_name)


# –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    async def test_parser():
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞."""
        async with RarityParser() as parser:
            items = await parser.fetch_all_rarities()
            
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(items)} –ø—Ä–µ–¥–º–µ—Ç–æ–≤")
            
            # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã Divine+ –ø—Ä–µ–¥–º–µ—Ç–æ–≤
            divine_items = await parser.get_divine_plus_items()
            print(f"\nDivine+ –ø—Ä–µ–¥–º–µ—Ç—ã ({len(divine_items)}):")
            for item in sorted(list(divine_items))[:10]:
                info = parser.get_item_info(item)
                if info:
                    print(f"  {info.rarity.value}: {item}")
    
    asyncio.run(test_parser())
